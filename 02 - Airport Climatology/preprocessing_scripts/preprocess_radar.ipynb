{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaination\n",
    "This script builds a condensed radar dataset at airport location\n",
    "\n",
    "### About the weather radar data\n",
    "Weather radar data is sourced from project rq0 on NCI. This data will soon be published via NCI (publically accessible by June 2020), but in the meantime you'll need to be an NCI user and request to join project rq0 to access the data. Data used for this exampled is processed 'level 2' data, which is on 1 x 1 km cartesian grid, rather than in the native radar coordinates (spherical). From the level 2 datasets, the 'reflectivity at 2km' and 'echo top heights' datasets are used. These consist of one netcdf file per radar per day per variable, whereby the dimensions are time, x and y.\n",
    "\n",
    "### What are extracting?\n",
    "In airport_table.csv, there is a radar identification number for each airport. For this radar, we are extracting the level 2 data within a search radius (in our case, 10 miles) from the airport. The maximum reflectivity is taken within this radius. A minimum threshold is also applied to remove precipitation that is likely not thunderstorms. If valid reflectivity of a thunderstorm exists within the search radius, the top of the reflectivity echo (proxy for storm top height) is also extracted. Our script also keeps track of which days there was no radar data.\n",
    "\n",
    "### What does this data mean?\n",
    "Reflectivity provides an indicator of thunderstorm presence and intensity\n",
    "Echo Top Height provides additional information on the depth of a thunderstorm (which relates to the intensity)\n",
    "\n",
    "To speed up processing 10 years of weather radar data, this notebook used the Multiprocessing library. 10 years of data (30GB) can be processed in 5 minutes using 15 cores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load startard libraries\n",
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import xarray as xr #used for reading netcdf files\n",
    "import numpy as np #used for arrays\n",
    "import pandas #used for reading csv files\n",
    "import tqdm #provides a nice progress bar for multiprocessing\n",
    "\n",
    "import pyart_transform #used for coordinate calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from l.\n",
    "    From http://stackoverflow.com/a/312464\n",
    "    \"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]\n",
    "\n",
    "def read_csv(csv_ffn, header_line):\n",
    "    \"\"\"\n",
    "    CSV reader used for the radar locations file (comma delimited)\n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "        csv_ffn: str\n",
    "            Full filename to csv file\n",
    "            \n",
    "        header_line: int or None\n",
    "            to use first line of csv as header = 0, use None to use column index\n",
    "            \n",
    "    Returns:\n",
    "    ========\n",
    "        as_dict: dict\n",
    "            csv columns are dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    df = pandas.read_csv(csv_ffn, header=header_line, skipinitialspace=True)\n",
    "    as_dict = df.to_dict(orient='list')\n",
    "    return as_dict\n",
    "\n",
    "def daterange(date1, date2):\n",
    "    \"\"\"\n",
    "    Generate date list between dates\n",
    "    \"\"\"\n",
    "    date_list = []\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        date_list.append(date1 + timedelta(n))\n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file patg config\n",
    "airport_csv_fn = 'airport_table.csv'\n",
    "out_folder = '../preprocessed_data/'\n",
    "radar_root = '/g/data/rq0/level_2/daily_150km'\n",
    "\n",
    "#date range\n",
    "start_date = '20090101'\n",
    "end_date = '20181231'\n",
    "\n",
    "#filters for data\n",
    "search_radius = 9260 #m, using 5 nautical mile radius\n",
    "min_reflectivity = 50 #dBZ\n",
    "min_eth = 5000 #m\n",
    "\n",
    "#build date list\n",
    "date_list  = daterange(datetime.strptime(start_date, '%Y%m%d'), datetime.strptime(end_date, '%Y%m%d'))\n",
    "\n",
    "#set number of CPU for multiprocessing\n",
    "NCPU = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_radar_data(radar_id, target_date, ap_lon, ap_lat, ap_name):\n",
    "    \n",
    "    \"\"\"\n",
    "    This is our core function that processes a single day of radar data and returns that requires statistics at the location of the airport\n",
    "    \n",
    "    Parameters:\n",
    "    ===========\n",
    "        radar_id: int\n",
    "            radar identification number\n",
    "            \n",
    "        target_date: datetime64\n",
    "            datetime value for target date\n",
    "        \n",
    "        ap_lon: float\n",
    "            value of airport longitude\n",
    "            \n",
    "        ap_lat: float\n",
    "            value of airport latitude\n",
    "        \n",
    "        ap_name: str\n",
    "            name of airport\n",
    "            \n",
    "    Returns:\n",
    "    ========\n",
    "        as_dict: dict\n",
    "            time series of statistics for target day\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    #convert to radar_id and target_date to strings for building file paths\n",
    "    radar_id_str = str(radar_id).zfill(2)\n",
    "    target_date_str = datetime.strftime(target_date, '%Y%m%d')\n",
    "    \n",
    "    #build file paths and check data files exist\n",
    "    var = 'ECHO_TOP_HEIGHTS'\n",
    "    eth_ffn = '/'.join([radar_root, var, radar_id_str, str(target_date.year)]) + '/' + '_'.join([radar_id_str, target_date_str, var]) + '.nc'\n",
    "    var = 'REFLECTIVITY'\n",
    "    ref_ffn = '/'.join([radar_root, var, radar_id_str, str(target_date.year)]) + '/' + '_'.join([radar_id_str, target_date_str, var]) + '.nc'\n",
    "    if not os.path.isfile(eth_ffn) or not os.path.isfile(ref_ffn):\n",
    "        #here we return the target_date, which is used to keep track of days missing data due to radar outage\n",
    "        return target_date\n",
    "    \n",
    "    #open weather radar datasets using xarray\n",
    "    ds_eth = xr.open_dataset(eth_ffn)\n",
    "    ds_ref = xr.open_dataset(ref_ffn)\n",
    "\n",
    "    #find location of ap in radar cartesian coordinate space (x,y)\n",
    "    radar_lat = float(ds_eth.origin_latitude)\n",
    "    radar_lon = float(ds_eth.origin_longitude)\n",
    "    ap_x, ap_y = pyart_transform.geographic_to_cartesian_aeqd(ap_lon, ap_lat, radar_lon, radar_lat)\n",
    "    \n",
    "    #using x,y dimensions, calculate distance of every grid point from ap_x and ap_y\n",
    "    x_grid, y_grid = np.meshgrid(ds_eth.x, ds_eth.y)\n",
    "    dist_grid = np.sqrt((x_grid-ap_x)**2 + (y_grid-ap_y)**2)\n",
    "    \n",
    "    #find points within the search radius distance\n",
    "    search_mask = dist_grid<search_radius\n",
    "\n",
    "    #extend mask into same coordinate space as netcdf data (repeat into a 3rd time dimension)\n",
    "    search_mask_time = np.repeat(search_mask[np.newaxis, :, :], len(ds_ref.time), axis=0)\n",
    "    \n",
    "    #apply mask to filter reflectivity and ETH\n",
    "    ds_ref_search = ds_ref.reflectivity.where(search_mask_time, other=0) #replace everything outside search radius with 0\n",
    "    ref_max = np.max(ds_ref_search, axis=(1,2))\n",
    "    ds_eth_search = ds_eth.echo_top_heights.where(search_mask_time, other=0) #replace everything outside search radius with 0\n",
    "    eth_max = np.max(ds_eth_search, axis=(1,2))\n",
    "    \n",
    "    #extract radar time\n",
    "    radar_time_daily = ds_ref.time.data\n",
    "\n",
    "    #threhold by reflectivity\n",
    "    valid_mask = np.logical_and(ref_max >= min_reflectivity, eth_max >= min_eth)\n",
    "    \n",
    "    #check if there's any valid data and return as dictionary\n",
    "    if np.any(valid_mask):\n",
    "        #return arrays\n",
    "        return {'time':radar_time_daily[valid_mask], 'ref':ref_max[valid_mask], 'eth':eth_max[valid_mask]}\n",
    "    else:\n",
    "        #no valid data, return nothing\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load airport list\n",
    "ap_dict = read_csv(airport_csv_fn, header_line=1)\n",
    "\n",
    "ap_name_list = ap_dict['Name']\n",
    "ap_lat_list = ap_dict['Latitude']\n",
    "ap_lon_list = ap_dict['Longitude']\n",
    "ap_rid_list = ap_dict['radar_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 68/243 [30:29<1:24:52, 29.10s/it]"
     ]
    }
   ],
   "source": [
    "#preprocess radar data!\n",
    "\n",
    "#for each airport\n",
    "for i, ap_name in enumerate(ap_name_list):\n",
    "    \n",
    "    #initalise variables to store our output data\n",
    "    radar_time = np.array([],dtype='datetime64')\n",
    "    radar_ref = np.array([])\n",
    "    radar_eth = np.array([])\n",
    "    radar_outage_dt = np.array([],dtype='datetime64')\n",
    "    \n",
    "    #extract data from ap csv\n",
    "    radar_id = ap_rid_list[i]\n",
    "    ap_lat = ap_lat_list[i]\n",
    "    ap_lon = ap_lon_list[i]\n",
    "    \n",
    "    #build chunked list for multiprocessing\n",
    "    chunked_list  = chunks(date_list, NCPU)\n",
    "    \n",
    "    #loop through dates using multiprocessing\n",
    "    for list_slice in tqdm.tqdm(chunked_list, total=int(len(date_list)/NCPU)):\n",
    "        #open multiprocessing pool\n",
    "        with Pool(NCPU) as pool:\n",
    "            #append additional arguments needed for core function\n",
    "            args_list = [(radar_id, oneset, ap_lon, ap_lat, ap_name) for oneset in list_slice]\n",
    "            #use starmap to handle multiple input function\n",
    "            result_list = pool.starmap(extract_radar_data, args_list)\n",
    "            #compile results\n",
    "            for result in result_list:\n",
    "                if result is None:\n",
    "                    #has returned no valid data\n",
    "                    continue\n",
    "                elif type(result) is dict:\n",
    "                    #has returned some data\n",
    "                    radar_time = np.append(radar_time, result['time'])\n",
    "                    radar_ref = np.append(radar_ref, result['ref'])\n",
    "                    radar_eth = np.append(radar_eth, result['eth'])\n",
    "                else:\n",
    "                    #has returned that there was a radar outage\n",
    "                    radar_outage_dt = np.append(radar_outage_dt, result)\n",
    "                    \n",
    "    #save to file\n",
    "    print('finished', ap_name)\n",
    "    save_path = out_folder + ap_name + '_radar.npz'\n",
    "    np.savez(save_path, radar_ref=radar_ref, radar_eth=radar_eth, radar_time=radar_time, radar_outage_dt=radar_outage_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__By extracted exactly what we need for analysis, we've converted a ~30 GB dataset into 4 x 0.05 MB files.__\n",
    "\n",
    "This will save us lots of time when analysing the data later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:radar-dev] *",
   "language": "python",
   "name": "conda-env-radar-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
